# MLOps Pipeline Example - Snowflake ML

A comprehensive example of MLOps best practices for deploying and managing Machine Learning pipelines on Snowflake using GitHub Actions, Snowpark, and Snowflake ML features (Feature Store, Model Registry).

## ğŸ“‹ Overview

This repository demonstrates an end-to-end ML pipeline for **customer churn prediction** that follows industry best practices for:

- âœ… **Environment Promotion**: DEV â†’ SIT â†’ UAT â†’ PRD
- âœ… **GitFlow Branching Strategy**: Feature branches, development, release, and main
- âœ… **Infrastructure as Code**: SQL scripts for Snowflake object management
- âœ… **CI/CD Automation**: GitHub Actions for automated deployments
- âœ… **Security**: Role-based access control (RBAC) and environment-specific service accounts
- âœ… **ML Best Practices**: Feature Store, Model Registry, and DAG-based orchestration

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GitHub Repository                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ feature/**   â”‚  â”‚ development  â”‚  â”‚  release/**  â”‚  main    â”‚
â”‚  â”‚   branches   â”‚  â”‚   branch     â”‚  â”‚   branches   â”‚  branch  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”€â”€â”€â”€â”¬â”€â”€â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”˜
          â”‚                  â”‚                  â”‚              â”‚
          â–¼                  â–¼                  â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DEV   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   SIT   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   UAT   â”‚â”€â”€â”€â–¶â”‚   PRD   â”‚
    â”‚ (XS WH) â”‚        â”‚  (S WH) â”‚       â”‚  (M WH) â”‚    â”‚  (L WH) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Auto Deploy        Auto Deploy       Auto Deploy    Manual Approval
```

### Pipeline Flow

1. **Feature Engineering** â†’ Reads raw data, transforms features, creates Feature View
2. **Model Training** â†’ Trains scikit-learn model using Feature Store data
3. **Model Registration** â†’ Registers model in Snowflake Model Registry
4. **Batch Inference** â†’ Runs predictions and saves to output table

All orchestrated via **Snowflake DAG** (Task Graph) with configurable scheduling.

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ snowflake_ml_deploy.yml    # CI/CD pipeline definition
â”‚   â””â”€â”€ actionlint.yaml                 # GitHub Actions linter config
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ environments.yml                # Environment-specific configurations
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy_pipeline.py              # DAG deployment script
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ml_logic.py                     # ML pipeline logic (feature eng, training, inference)
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 00_setup_warehouses.sql         # Warehouse creation
â”‚   â”œâ”€â”€ 01_setup_dev_environment.sql    # DEV environment setup
â”‚   â”œâ”€â”€ 02_setup_sit_environment.sql    # SIT environment setup
â”‚   â”œâ”€â”€ 02b_setup_uat_environment.sql   # UAT environment setup
â”‚   â”œâ”€â”€ 03_setup_prd_environment.sql    # PRD environment setup
â”‚   â”œâ”€â”€ 04_setup_roles_and_grants.sql   # RBAC setup
â”‚   â”œâ”€â”€ 99_setup_all_environments.sql   # All-in-one setup script
â”‚   â””â”€â”€ README.md                       # SQL scripts documentation
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ BRANCH_PROTECTION.md            # Branch protection setup guide
â”‚
â”œâ”€â”€ requirements.txt                     # Python dependencies
â””â”€â”€ README.md                           # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Snowflake account with ACCOUNTADMIN access
- GitHub repository with Actions enabled
- Python 3.12+ (for local development)
- Snowflake CLI installed (optional, for local testing)

### Step 1: Setup Snowflake Infrastructure

Execute SQL scripts in order (in Snowsight or via SnowSQL):

```bash
# Option A: Individual scripts (recommended for production)
sql/00_setup_warehouses.sql         # Creates compute warehouses
sql/01_setup_dev_environment.sql    # DEV environment
sql/02_setup_sit_environment.sql    # SIT environment
sql/02b_setup_uat_environment.sql   # UAT environment
sql/03_setup_prd_environment.sql    # PRD environment
sql/04_setup_roles_and_grants.sql   # Security & RBAC

# Option B: All-in-one (good for demos)
sql/99_setup_all_environments.sql   # Creates everything at once
```

### Step 2: Configure GitHub Secrets

Navigate to **Settings â†’ Secrets and variables â†’ Actions** and add:

```yaml
# Common secrets (used across all environments)
SNOWFLAKE_ACCOUNT: "<your_account_identifier>"
SNOWFLAKE_USER: "<github_actions_service_account>"
SNOWFLAKE_PRIVATE_KEY: "<private_key_content>"

# Environment-specific role secrets
SNOWFLAKE_DEV_ROLE: "ML_DEV_ROLE"
SNOWFLAKE_SIT_ROLE: "ML_SIT_ROLE"
SNOWFLAKE_UAT_ROLE: "ML_UAT_ROLE"
SNOWFLAKE_PRD_ROLE: "ML_PRD_ROLE"
```

### Step 3: Setup Branch Protection

Follow instructions in [`docs/BRANCH_PROTECTION.md`](./docs/BRANCH_PROTECTION.md) to configure:

- Required pull request reviews
- Status checks before merging
- Manual approval environment for production

### Step 4: Deploy Your First Pipeline

```bash
# Create a feature branch
git checkout -b feature/initial-pipeline

# Make changes and push
git add .
git commit -m "Initial ML pipeline setup"
git push origin feature/initial-pipeline

# GitHub Actions will automatically deploy to DEV
# Check Actions tab for deployment status
```

## ğŸ”„ Branching & Deployment Strategy

This repository follows **Enhanced GitFlow** with environment promotion:

| Branch Pattern | Environment | Auto-Deploy | Approval Required | DAG Schedule |
|----------------|-------------|-------------|-------------------|--------------|
| `feature/**`   | DEV         | âœ… Yes      | âŒ No             | Suspended    |
| `development`  | SIT         | âœ… Yes      | âŒ No             | Suspended    |
| `release/**`   | UAT         | âœ… Yes      | âŒ No             | Suspended    |
| `main`         | PRD         | âœ… Yes      | âœ… **Yes**        | Active (24h) |

### Workflow

1. **Feature Development**: Create `feature/your-feature` â†’ Auto-deploys to DEV
2. **Integration Testing**: Merge to `development` â†’ Auto-deploys to SIT
3. **Pre-Production**: Create `release/v1.0.0` â†’ Auto-deploys to UAT
4. **Production**: Merge release to `main` â†’ **Requires manual approval** â†’ Deploys to PRD

### Manual Approval for Production

Production deployments require approval from designated reviewers:

1. Configure GitHub Environment: **Settings â†’ Environments â†’ production**
2. Add required reviewers
3. Set environment protection rules

See [`docs/BRANCH_PROTECTION.md`](./docs/BRANCH_PROTECTION.md) for detailed setup.

## ğŸ§ª Environment Details

| Environment | Purpose | Warehouse | Data Volume | Retention |
|-------------|---------|-----------|-------------|-----------|
| **DEV** | Feature development & experimentation | XS | 1K records | 1 day |
| **SIT** | Integration testing | Small | 5K records | 1 day |
| **UAT** | Pre-production validation | Medium | 10K records | 3 days |
| **PRD** | Production workloads | Large | Production data | 7 days |

### Snowflake Objects Created per Environment

- **Databases**: `{ENV}_RAW_DB`, `{ENV}_ML_DB`
- **Schemas**: `PIPELINES`, `FEATURES`, `OUTPUT`
- **Stages**: `ML_CODE_STAGE`, `MODELS_STAGE`
- **Tables**: `CUSTOMERS` (raw), `CUSTOMER_FEATURES` (feature store), `PREDICTIONS` (output)
- **Roles**: `ML_{ENV}_ROLE`

## ğŸ” Security & Access Control

### Service Accounts

Each environment uses a separate role for isolation:

- `ML_DEV_ROLE` - Full access to DEV
- `ML_SIT_ROLE` - Full access to SIT
- `ML_UAT_ROLE` - Full access to UAT
- `ML_PRD_ROLE` - Controlled access to PRD (read-only on raw data)
- `ML_CICD_ROLE` - Inherits all roles for CI/CD deployments

### Creating CI/CD Service Account

```sql
USE ROLE ACCOUNTADMIN;

-- Generate key-pair locally first:
-- openssl genrsa -out snowflake_key.pem 2048
-- openssl rsa -in snowflake_key.pem -pubout -out snowflake_key.pub

CREATE USER github_actions_user
    RSA_PUBLIC_KEY = '<paste_public_key_content>'
    DEFAULT_ROLE = ML_CICD_ROLE
    MUST_CHANGE_PASSWORD = FALSE;

GRANT ROLE ML_CICD_ROLE TO USER github_actions_user;
```

## ğŸ›  Local Development

### Setup

```bash
# Clone repository
git clone <repo_url>
cd MLOps_example

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Test Locally

```bash
# Set environment variables
export SNOWFLAKE_ACCOUNT="<your_account>"
export SNOWFLAKE_USER="<your_user>"
export SNOWFLAKE_PRIVATE_KEY="<your_private_key>"
export SNOWFLAKE_ROLE="ML_DEV_ROLE"
export SNOWFLAKE_WAREHOUSE="DEV_WH_XS"
export SNOWFLAKE_DATABASE="DEV_ML_DB"
export SNOWFLAKE_SCHEMA="PIPELINES"

# Run deployment script
python scripts/deploy_pipeline.py DEV
```

## ğŸ“Š Monitoring & Validation

### Check DAG Status

```sql
-- View deployed DAGs
SHOW TASKS IN SCHEMA DEV_ML_DB.PIPELINES;

-- Check DAG execution history
SELECT *
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(
    SCHEDULED_TIME_RANGE_START => DATEADD('day', -7, CURRENT_TIMESTAMP())
))
WHERE NAME LIKE 'ML_RETRAINING_PIPELINE%'
ORDER BY SCHEDULED_TIME DESC;
```

### Monitor Predictions (PRD only)

```sql
USE SCHEMA PRD_ML_DB.OUTPUT;

-- Recent predictions
SELECT * FROM RECENT_PREDICTIONS LIMIT 100;

-- Prediction statistics
SELECT * FROM PREDICTION_STATS;
```

## ğŸ“š Key Components

### 1. ML Logic (`src/ml_logic.py`)

Three main functions:

- `feature_engineering_task()` - Creates Feature Store with Entity and Feature View
- `model_training_task()` - Trains and registers model in Model Registry
- `inference_task()` - Runs batch predictions

### 2. Deployment Script (`scripts/deploy_pipeline.py`)

- Reads environment config from `config/environments.yml`
- Creates Snowpark session with key-pair auth
- Defines DAG with three tasks and dependencies
- Deploys to Snowflake using DAGOperation API

### 3. CI/CD Workflow (`.github/workflows/snowflake_ml_deploy.yml`)

- Triggers on push to `feature/**`, `development`, `release/**`, `main`
- Determines target environment based on branch
- Uploads Python code to Snowflake stage
- Executes deployment script
- Requires manual approval for production

## âš ï¸ Limitations & Recommendations for Production

This is an **example/reference implementation** designed for learning and demonstration. For production-grade MLOps, consider implementing:

### ğŸ”´ Critical Gaps

1. **No Automated Testing**
   - âŒ Missing unit tests for ML logic
   - âŒ No integration tests for Snowflake connectivity
   - âŒ No data validation tests
   - ğŸ’¡ **Recommendation**: Add pytest with Snowpark testing, data quality checks (Great Expectations), and model validation tests

2. **No Model Validation Gates**
   - âŒ Models deploy without performance validation
   - âŒ No A/B testing or champion/challenger comparison
   - ğŸ’¡ **Recommendation**: Implement model performance thresholds, drift detection, and automated rollback on degradation

3. **Limited Monitoring & Observability**
   - âŒ No model performance monitoring in production
   - âŒ No drift detection (data/concept drift)
   - âŒ No alerting on pipeline failures
   - ğŸ’¡ **Recommendation**: Integrate with monitoring tools (DataDog, Evidently AI), add custom metrics tables, implement SLO/SLA tracking

4. **No Rollback Strategy**
   - âŒ Failed deployments require manual intervention
   - âŒ No versioning for DAG definitions
   - ğŸ’¡ **Recommendation**: Implement blue-green deployments, maintain model version history, automated rollback on failure

### ğŸŸ¡ Important Enhancements

5. **Basic Error Handling**
   - âš ï¸ Limited exception handling in ML logic
   - âš ï¸ No retry logic for transient failures
   - ğŸ’¡ **Recommendation**: Add comprehensive error handling, implement exponential backoff, dead letter queues for failed records

6. **Simplified Feature Engineering**
   - âš ï¸ Basic feature transformations only
   - âš ï¸ No feature validation or schema enforcement
   - ğŸ’¡ **Recommendation**: Implement feature contracts, add data quality checks, use Snowflake's data quality functions

7. **Single Model Architecture**
   - âš ï¸ Only supports one model at a time
   - âš ï¸ No ensemble or multi-model support
   - ğŸ’¡ **Recommendation**: Support model ensembles, A/B testing infrastructure, shadow mode deployments

8. **Limited Security Hardening**
   - âš ï¸ Service account uses single key-pair
   - âš ï¸ No secrets rotation policy
   - âš ï¸ No audit logging for model changes
   - ğŸ’¡ **Recommendation**: Implement secrets rotation, enable Snowflake audit logs, add change management tracking

9. **No Cost Optimization**
   - âš ï¸ Warehouses may run longer than needed
   - âš ï¸ No automatic scaling based on workload
   - ğŸ’¡ **Recommendation**: Implement auto-suspend policies, use query tags for cost attribution, optimize warehouse sizing

10. **Documentation & Governance**
    - âš ï¸ No model cards or documentation
    - âš ï¸ Limited lineage tracking
    - âš ï¸ No compliance/regulatory considerations
    - ğŸ’¡ **Recommendation**: Implement model cards, use Snowflake's object tagging for lineage, add compliance metadata

### ğŸŸ¢ Nice-to-Have Features

11. **Advanced ML Features**
    - Feature store versioning and time-travel
    - Hyperparameter tuning automation
    - Online feature serving (if needed)
    - Model explainability (SHAP, LIME)

12. **Developer Experience**
    - Pre-commit hooks for code quality
    - Local development environment with Docker
    - Jupyter notebooks for experimentation
    - VS Code/Cursor integration guides

13. **Advanced CI/CD**
    - Canary deployments
    - Progressive rollouts
    - Automated performance benchmarking
    - Integration with Jira/project management

## ğŸ¤ Contributing

This is an example repository for learning purposes. Feel free to fork and adapt to your needs!

## ğŸ“„ License

This example is provided as-is for educational purposes.

## ğŸ™‹ Support & Resources

- [Snowflake ML Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)
- [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview)
- [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

---

**Last Updated**: 2025-11-25  
**Version**: 1.0  
**Snowflake ML SDK**: Compatible with latest Snowpark ML Python SDK

